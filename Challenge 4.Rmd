---
title: "Challenge 4"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(amerika)
library(tictoc)
library(h2o)
library(bit64)
library(scales)

data <- read_csv(".\\data\\anes_2016.csv")

set.seed(11235)
my_h2o <- h2o.init()


#issues <- data %>%
 #   dplyr::select(vaccine, autism, birthright_b, forceblack, forcewhite, stopblack, stopwhite, freetrade, aa3, warmdo, finwell, childcare, healthspend, minwage) %>%
  #scale()%>%
  #as_tibble()


issues <- data %>%
    dplyr::select(vaccine, autism, birthright_b, forceblack, forcewhite, stopblack, stopwhite, freetrade, aa3, warmdo, finwell, childcare, healthspend, minwage) %>%
  scale(center = TRUE, scale = TRUE)%>%  
  as_tibble()%>%
  mutate_all(rescale, to = c(0,1))


anes <- data %>%
  as_tibble()%>%
  mutate(strong_amer_ident = factor(if_else(amer_ident %in% c(1,2), 1, 0)),
         strong_race_ident = factor(if_else(race_ident  %in% c(1,2), 1, 0)))

issues$strong_amer_ident <- anes$strong_amer_ident
issues$strong_race_ident <- anes$strong_race_ident

predictors <- colnames(issues)[-(15:16)]
```

# The American Identity

1. (10 points) Build a shallow autoencoder with a single hidden layer consisting of 2 nodes on the full question space, but not including the dichotomous American identity feature. Then, extract the two “deep” features from the hidden layer and store these.

```{r}
amer_h2o <- issues[-16] %>%
  as.h2o()

shallow_autoencoder_amer <- h2o.deeplearning(x = predictors,
                                training_frame = amer_h2o,
                                autoencoder = TRUE,
                                hidden = c(2),
                                epochs = 100,
                                activation = "Tanh")

shallow_features_amer <- h2o.deepfeatures(shallow_autoencoder_amer, 
                            data = amer_h2o, 
                            layer = 1)%>%
  as.data.frame()%>%
  mutate(strong_amer_ident = as.vector(amer_h2o[,15]))

```

2. (10 points) Plot the two deep features against each other, with color conditioned by weak or strong American identity. Discuss the output in a few sentences. For example, do we see separation in the projection (question) space along senses of American identity or not? Why or why not do you think?

There is some spatial sorting; for instance it seems that negative values of feature 1 (along the x-axis) is associated strong american identity. But by no means is this a clear distinction. There is no true separation to speak of. This could indicate that processing the survey responses with two neurons in one layer doesn't enable the model to detect strong/weak senses of american identity. This could be because of a weak underlying relationship between the survey variables and sense of American identity, or it could be because 2 neurons in one layer doesn't give the model enough flexibility to detect the separation.

```{r}
ggplot(shallow_features_amer, aes(x=DF.L1.C1, 
                          y = DF.L1.C2, 
                          color = factor(strong_amer_ident)))+
  geom_point(alpha = 0.5) +
  stat_ellipse() +
  labs(title = "Strong Feelings of American Identity")
  

```
```{r}
h2o.anomaly(shallow_autoencoder_amer, amer_h2o) %>% 
  as.data.frame() %>% 
  ggplot(aes(Reconstruction.MSE)) +
  geom_histogram() + 
  theme_minimal()
```



3. (10 points) Build a deep autoencoder with 3 hidden layers consisting of 2 nodes in each on the full question space, but again not including the dichotomous American identity feature. Then, extract the two deep features from the third hidden layer and store these.

```{r}
deep_autoencoder_amer <- h2o.deeplearning(x = predictors,
                                training_frame = amer_h2o,
                                autoencoder = TRUE,
                                hidden = c(2, 2, 2),
                                epochs = 100,
                                activation = "Tanh")

deep_features_amer <- h2o.deepfeatures(deep_autoencoder_amer, 
                            data = amer_h2o, 
                            layer = 3)%>%
  as.data.frame()%>%
  mutate(strong_amer_ident = as.vector(amer_h2o[,15]))
```

4. (10 points) Plot the two deep features from the 3rd hidden layer against each other, with color conditioned by weak or strong American identity. Discuss the output in a few sentences. For example, does deepening the network help to recover different patterns and/or clearer separation in the question space along this identity? Why or why not do you think? What do we gain and what do we lose by deepening the network?

The additional neuron layers did not noticeably increase separation. To my eyes, this graph looks very similar to the previous one, but rotated. There is still some sorting of strong/weak american identity, this time positive values of DF.L3.C2 are associated with strong American identity. Perhaps this is because when we compress the data down to two neurons in the first layer, the subsequent 2-neuron layers can't make large improvments in pattern detection. In theory, deepening the network enables the model to identify more complex nonlinear patterns more efficiently. But it does so at the expense of greater complexity and greater computational burden.

```{r}
ggplot(deep_features_amer, aes(x=DF.L3.C1, 
                          y = DF.L3.C2, 
                          color = factor(strong_amer_ident)))+
  geom_point() +
  stat_ellipse()
```
```{r}
h2o.anomaly(deep_autoencoder_amer, amer_h2o) %>% 
  as.data.frame() %>% 
  ggplot(aes(Reconstruction.MSE)) +
  geom_histogram() + 
  theme_minimal()
```


# The Racial Identity
5. (10 points) Build a shallow autoencoder with a single hidden layer consisting of 2 nodes on the full question space, but not including the dichotomous race identity feature. Then, extract the two “deep” features from the hidden layer and store these.

```{r}
race_h2o <- issues[-15] %>%
  as.h2o()

shallow_autoencoder_race <- h2o.deeplearning(x = predictors,
                                training_frame = race_h2o,
                                autoencoder = TRUE,
                                hidden = c(2),
                                epochs = 100,
                                activation = "Tanh")

shallow_features_race <- h2o.deepfeatures(shallow_autoencoder_race, 
                            data = race_h2o, 
                            layer = 1)%>%
  as.data.frame()%>%
  mutate(strong_race_ident = as.vector(race_h2o[,15]))
```

6. (10 points) Plot the two deep features against each other, with color conditioned by weak or strong racial identity. Discuss the output in a few sentences. For example, do we see separation in the projection (question) space along senses of racial-identity or not? Why or why not do you think?

For racial identity, there seems to be no separation, and barely any sorting. The ellipses almost entirely overlap. This shows that the survey responses are not able to reconstruct sense of racial identity, possibly because racial identity just isn't very predictive of responses to the survey questions.


```{r}
ggplot(shallow_features_race, aes(x=DF.L1.C1, 
                          y = DF.L1.C2, 
                          color = factor(strong_race_ident)))+
  geom_point() +
  stat_ellipse()
```
```{r}
h2o.anomaly(shallow_autoencoder_race, race_h2o) %>% 
  as.data.frame() %>% 
  ggplot(aes(Reconstruction.MSE)) +
  geom_histogram() + 
  theme_minimal()
```

7. (10 points) Build a deep autoencoder with 3 hidden layers consisting of 2 nodes in each on the full question space, but again not including the dichotomous racial identity feature. Then, extract the two deep features from the third hidden layer and store these.

```{r}
deep_autoencoder_race <- h2o.deeplearning(x = predictors,
                                training_frame = race_h2o,
                                autoencoder = TRUE,
                                hidden = c(2, 2, 2),
                                epochs = 100,
                                activation = "Tanh")

deep_features_race <- h2o.deepfeatures(deep_autoencoder_race, 
                            data = race_h2o, 
                            layer = 3)%>%
  as.data.frame()%>%
  mutate(strong_race_ident = as.vector(race_h2o[,15]))
```



8. (10 points) Plot the two deep features from the 3rd hidden layer against each other, with color conditioned by weak or strong racial identity. Discuss the output in a few sentences. For example, does deepening the network help to recover different patterns and/or clearer separation in the question space along this identity? Why or why not do you think? What do we gain and what do we lose by deepening the network?

Deepening the network seems like it might have marginally increased sorting (I would hesitate to call this "separation"). The change is subtle, but there definitely appears to be a cluster of strong race identity in quadrant IV. This could reflect the fact that the deeper neural network is better able to locate the structural components of these questions that also produce separation for strong racial identity. However, interpretability remains a challenge; it's hard to tell exactly what the deep structure is that the model has picked up.

```{r}
ggplot(deep_features_race, aes(x=DF.L3.C1, 
                          y = DF.L3.C2, 
                          color = factor(strong_race_ident)))+
  geom_point() +
  stat_ellipse()
```

```{r}
h2o.anomaly(deep_autoencoder_race, race_h2o) %>% 
  as.data.frame() %>% 
  ggplot(aes(Reconstruction.MSE)) +
  geom_histogram() + 
  theme_minimal()
```

# Self-Organizing Maps vs. Autoencoders
9. (20 points) Compare the patterns across these two identities - American and racial - to the patterns found from last week’s challenge using self-organizing maps. Are the patterns similar or different across these techniques (SOM vs. AE)? Why do you think? What might the benefit be of picking one of these neural network-based approaches to dimension reduction over the other? What do we gain with such a choice and what do we lose? And so on. 7-10 well-constructed sentences should suffice.

In my SOM analysis, I found some amount of clustering for the American Identity, and a lesser amount for racial identity. This is roughly mirrored in my analysis of the two variables using AE, but a deep AE approach found some hint of clustering, which was not the case in my SOM analysis.

The benefit of the AE approach over SOM is that it is able to operate efficiently in non-linear spaces. The AE reconstruction MSEs were reasonably low, suggesting that although the AE wasn't able to find clustering in the identify variables, it may have found some useful underlying patterns.

One benefit of SOM was that I was able to graph heat maps for individual variables from the input data, which helped me understand the relationship between different survey questions. With the AE approach, I don't have a simple way to understand those relationships.

Taken together, the fact taht my findings from this AE analysis seem to back my findings from the previous SOM analysis gives my conclusions from last week stronger: although there is slightly more evidence of clustering by feelings of American identity than there is for clustering by feelings of racial identity, the clustering is weak overall, reflecting a lower-than-expected strength of these identity variables in determining other political views.